{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. KNN算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 贝叶斯算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 决策树算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. SVM算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 主要的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 线性回归算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 主要思想：\n",
    "最小二乘法，用于线性拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 岭回归算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 主要思想：\n",
    "岭回归就是在矩阵xtx上加一个λz从而使得矩阵非奇异，进而能对xtx+λz求逆。岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计加入偏差，从而得到更好的估计。这里通过引入惩罚项，能够减少不重要的参数。\n",
    "- 优点：\n",
    "缩减方法可以去掉不重要的参数，因此能更好的理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。\n",
    "主要适用于过拟合严重或各变量之间在多重共线性的时候。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# alpha : αα值，其值越大则正则化项的占比越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Lasso算法描述，sklearn参数说明："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 主要思想：\n",
    "和岭回归相似，它也是通过增加惩罚函数来判断、消除特征间的共线性。\n",
    "- 优点：\n",
    "缩减方法可以去掉不重要的参数，因此能更好的理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。\n",
    "主要适用于过拟合严重或各变量之间在多重共线性的时候。\n",
    "- 参数说明：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# C : 一个浮点数。它指定了罚项系数的倒数。如果它的值越小，则正则化项越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Logistic算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 主要思想：\n",
    "根据现有数据对分类边界建立回归公式，找到最佳拟合参数集，以此分类。\n",
    "- 优缺点：\n",
    "优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低；\n",
    "缺点：容易欠拟合，分类精度可能不高。\n",
    "- 参数说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class sklearn.linear_model.Lasso(alpha=1.0,fit_intercept=True,normalize=False,precompute=False, \n",
    "copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None, \n",
    "selection=’cyclic’)\n",
    "\n",
    "# 重要参数\n",
    "# alpha : αα值，其值越大则正则化项的占比越大。\n",
    "# warm_start : 一个布尔值。如果为True，那么使用前一次训练结果继续训练。否则从头开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. KMeans算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. PCA算法描述，sklearn参数说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
